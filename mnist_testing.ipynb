{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000, 10)\n",
      "<TensorSliceDataset shapes: ((28, 28), (10,)), types: (tf.float32, tf.float32)>\n",
      "<BatchDataset shapes: ((None, 28, 28), (None, 10)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.titlesize'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['STKaiTi']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "# 数据读取和预处理\n",
    "# 1. 读取训练集、测试集数据\n",
    "# 2. 对数据进行预处理。X数据（像素灰度）进行归一化处理，一般处理到[0, 1]或[-1, 1]之间\n",
    "#    对Y数据（分类标签）进行one-hot编码，去掉数字标签可能带来的大小关系\n",
    "# 3. 使用tf.data.Dataset.from_tensor_slice((x, y)) 将三维的图片数据，按照第一个维度进行展开，及进行“打平”操作\n",
    "# 4. 使用tf.data.Dataset.from_tensor_slice.batch 方法，设置批处理数据的大小\n",
    "(x, y), (x_val, y_val) = tf.keras.datasets.mnist.load_data() #  如果没有from ... import ... 语句，需要一层层引用\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32)/255. # 转换数据范围到[0,1]\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "y = tf.one_hot(y, depth=10) # one-hot 编码，去掉标签的大小关系\n",
    "print(x.shape, y.shape)\n",
    "# tf.data.Dataset.from_tensor_slices真正作用是切分传入Tensor的第一个维度，生成相应的dataset，即第一维表明数据集中数据的数量，之后切分batch等操作都以第一维为基础。\n",
    "# 打平后的数据维度为((28, 28)图片, (10, )标记)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)) \n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.batch(600) # 暂：转换完的维度(None, 28, 28)中None\n",
    "#train_dataset = train_dataset.repeat(20) # 如果用for epoch in range(20)，则不用这一句，效果一样\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络搭建\n",
    "# 1. 使用Sequential容器，搭建3曾网络如下\n",
    "# 2. 需要乡下一层传递时，需使用activation函数\n",
    "# 3. 输出层因不用向下一层传递，因此不用激活函数activation\n",
    "model = keras.Sequential([ # 三个非线性层的嵌套模型，包括两个隐藏层，一个输出层\n",
    "        layers.Dense(256, activation='relu'), # 隐藏层1，用relu作为激活函数\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(10)]) # 输出层不用激活函数，输出节点数为10\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optimizers.SGD(learning_rate=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 1.6420572\n",
      "0 20 loss: 0.58835083\n",
      "0 40 loss: 0.46164078\n",
      "0 60 loss: 0.37137765\n",
      "0 80 loss: 0.29022345\n",
      "1 0 loss: 0.2672438\n",
      "1 20 loss: 0.29224572\n",
      "1 40 loss: 0.24534388\n",
      "1 60 loss: 0.2422368\n",
      "1 80 loss: 0.2075305\n",
      "2 0 loss: 0.19581568\n",
      "2 20 loss: 0.21875928\n",
      "2 40 loss: 0.19606483\n",
      "2 60 loss: 0.19903563\n",
      "2 80 loss: 0.17718397\n",
      "3 0 loss: 0.1662726\n",
      "3 20 loss: 0.18863627\n",
      "3 40 loss: 0.16992478\n",
      "3 60 loss: 0.17534819\n",
      "3 80 loss: 0.15844448\n",
      "4 0 loss: 0.14903355\n",
      "4 20 loss: 0.17009947\n",
      "4 40 loss: 0.15294532\n",
      "4 60 loss: 0.15936646\n",
      "4 80 loss: 0.14518616\n",
      "5 0 loss: 0.13706009\n",
      "5 20 loss: 0.15673381\n",
      "5 40 loss: 0.14061546\n",
      "5 60 loss: 0.1472569\n",
      "5 80 loss: 0.13465223\n",
      "6 0 loss: 0.12803301\n",
      "6 20 loss: 0.1463935\n",
      "6 40 loss: 0.1309917\n",
      "6 60 loss: 0.13777399\n",
      "6 80 loss: 0.1260041\n",
      "7 0 loss: 0.12079926\n",
      "7 20 loss: 0.13784757\n",
      "7 40 loss: 0.12301956\n",
      "7 60 loss: 0.13025157\n",
      "7 80 loss: 0.1187753\n",
      "8 0 loss: 0.11473055\n",
      "8 20 loss: 0.13069613\n",
      "8 40 loss: 0.11634931\n",
      "8 60 loss: 0.123904645\n",
      "8 80 loss: 0.112533785\n",
      "9 0 loss: 0.10954019\n",
      "9 20 loss: 0.12440041\n",
      "9 40 loss: 0.11054535\n",
      "9 60 loss: 0.118453346\n",
      "9 80 loss: 0.10716233\n",
      "10 0 loss: 0.10497082\n",
      "10 20 loss: 0.118976176\n",
      "10 40 loss: 0.10556895\n",
      "10 60 loss: 0.113553986\n",
      "10 80 loss: 0.10236538\n",
      "11 0 loss: 0.101020485\n",
      "11 20 loss: 0.11406679\n",
      "11 40 loss: 0.10116366\n",
      "11 60 loss: 0.10934689\n",
      "11 80 loss: 0.09809338\n",
      "12 0 loss: 0.09759356\n",
      "12 20 loss: 0.1097743\n",
      "12 40 loss: 0.097080685\n",
      "12 60 loss: 0.10556635\n",
      "12 80 loss: 0.0943149\n",
      "13 0 loss: 0.09449197\n",
      "13 20 loss: 0.10589224\n",
      "13 40 loss: 0.09355193\n",
      "13 60 loss: 0.10198235\n",
      "13 80 loss: 0.09090609\n",
      "14 0 loss: 0.09168474\n",
      "14 20 loss: 0.102403216\n",
      "14 40 loss: 0.09035341\n",
      "14 60 loss: 0.09882968\n",
      "14 80 loss: 0.08787391\n",
      "15 0 loss: 0.08914632\n",
      "15 20 loss: 0.0992348\n",
      "15 40 loss: 0.08752725\n",
      "15 60 loss: 0.09599728\n",
      "15 80 loss: 0.08511785\n",
      "16 0 loss: 0.0867305\n",
      "16 20 loss: 0.09629338\n",
      "16 40 loss: 0.084851645\n",
      "16 60 loss: 0.09340745\n",
      "16 80 loss: 0.0826829\n",
      "17 0 loss: 0.084367245\n",
      "17 20 loss: 0.09355482\n",
      "17 40 loss: 0.08231012\n",
      "17 60 loss: 0.09107282\n",
      "17 80 loss: 0.08037862\n",
      "18 0 loss: 0.082248755\n",
      "18 20 loss: 0.09101301\n",
      "18 40 loss: 0.079957664\n",
      "18 60 loss: 0.088890314\n",
      "18 80 loss: 0.07826922\n",
      "19 0 loss: 0.080305465\n",
      "19 20 loss: 0.0885953\n",
      "19 40 loss: 0.07782688\n",
      "19 60 loss: 0.08681812\n",
      "19 80 loss: 0.07626042\n",
      "20 0 loss: 0.07848575\n",
      "20 20 loss: 0.08631819\n",
      "20 40 loss: 0.07591189\n",
      "20 60 loss: 0.08489269\n",
      "20 80 loss: 0.07454033\n",
      "21 0 loss: 0.07683954\n",
      "21 20 loss: 0.08407877\n",
      "21 40 loss: 0.07407523\n",
      "21 60 loss: 0.0831032\n",
      "21 80 loss: 0.072986655\n",
      "22 0 loss: 0.07529359\n",
      "22 20 loss: 0.082024194\n",
      "22 40 loss: 0.07237348\n",
      "22 60 loss: 0.081339136\n",
      "22 80 loss: 0.07145086\n",
      "23 0 loss: 0.07374697\n",
      "23 20 loss: 0.08014322\n",
      "23 40 loss: 0.070789844\n",
      "23 60 loss: 0.079743885\n",
      "23 80 loss: 0.069935545\n",
      "24 0 loss: 0.072330296\n",
      "24 20 loss: 0.07833811\n",
      "24 40 loss: 0.069297284\n",
      "24 60 loss: 0.07822229\n",
      "24 80 loss: 0.06855185\n",
      "25 0 loss: 0.07097987\n",
      "25 20 loss: 0.076696984\n",
      "25 40 loss: 0.06784389\n",
      "25 60 loss: 0.0767819\n",
      "25 80 loss: 0.06727855\n",
      "26 0 loss: 0.06971273\n",
      "26 20 loss: 0.07510157\n",
      "26 40 loss: 0.06646619\n",
      "26 60 loss: 0.07544411\n",
      "26 80 loss: 0.06603551\n",
      "27 0 loss: 0.06842837\n",
      "27 20 loss: 0.07355258\n",
      "27 40 loss: 0.065133035\n",
      "27 60 loss: 0.07413967\n",
      "27 80 loss: 0.06481524\n",
      "28 0 loss: 0.067221366\n",
      "28 20 loss: 0.0720565\n",
      "28 40 loss: 0.06385603\n",
      "28 60 loss: 0.07289754\n",
      "28 80 loss: 0.063636094\n",
      "29 0 loss: 0.06606603\n",
      "29 20 loss: 0.0706177\n",
      "29 40 loss: 0.06266861\n",
      "29 60 loss: 0.071745545\n",
      "29 80 loss: 0.06258596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"另一种实现方式Dataset.repeat()   没搞懂！！！\\n    # 第四步： 循环迭代优化\\nfor step, (x, y) in enumerate(train_dataset): \\n    with tf.GradientTape() as tape: # python 语法，对于后续需要释放的资源，使用with...as...语句，避免忘记释放资源\\n        x = tf.reshape(x, (-1, 28*28)) # 打平操作\\n        out = model(x) # 第一步：得到模型输出, 相当于表达式中的Y\\n        # 第二步：计算平均误差\\n        loss = tf.square(out - y) # 计算平方和[b, 10]\\n        loss = tf.reduce_sum(loss) / x.shape[0] # 计算每个样本的平均误差[b]\\n        # 第三步：计算并优化参数[w1, w2, w3, b1, b2, b3]\\n        grads = tape.gradient(loss, model.trainable_variables) # 自动计算梯度\\n        optimizer.apply_gradients(zip(grads, model.trainable_variables)) # w'=w-lr*grad, 更新网络参数\\n        # 每500次计算绘制一次图像\\n#        if step % 100 == 0: \\n#            print(epoch, step, 'loss:', loss.numpy())\\n    print(step, 'loss:', loss.numpy())\\nlosses.append(float(loss))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "\n",
    "losses = []\n",
    "# ''' 用epoch和for循环实现迭代训练\n",
    "for epoch in range(30):\n",
    "    # 第四步： 循环迭代优化\n",
    "    for step, (x, y) in enumerate(train_dataset): # python 语法，enumerate()方法将train_dataset中的后两维数据以枚举形式付给step对应的(x, y)变量\n",
    "        with tf.GradientTape() as tape: # python 语法，对于后续需要释放的资源，使用with...as...语句，避免忘记释放资源\n",
    "            # ？？？\n",
    "            x = tf.reshape(x, (-1, 28*28)) # 打平操作\n",
    "            out = model(x) # 第一步：得到模型输出, 相当于表达式中的Y\n",
    "            # 第二步：计算平均误差\n",
    "            loss = tf.square(out - y) # 计算平方和[b, 10]\n",
    "            loss = tf.reduce_sum(loss) / x.shape[0] # 计算每个样本的平均误差[b]\n",
    "            # 第三步：计算并优化参数[w1, w2, w3, b1, b2, b3]\n",
    "            grads = tape.gradient(loss, model.trainable_variables) # 自动计算梯度\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables)) # w'=w-lr*grad, 更新网络参数\n",
    "        # 每500次计算绘制一次图像\n",
    "        if step % 20 == 0: \n",
    "            print(epoch, step, 'loss:', loss.numpy())\n",
    "        # print(epoch, step, 'loss:', loss.numpy()) # step = 数据第一维度数量（图片张数）/ n； 最大值为 batch(n) 中的n\n",
    "    losses.append(float(loss))\n",
    "# '''\n",
    "\n",
    "'''另一种实现方式Dataset.repeat()   没搞懂！！！\n",
    "    # 第四步： 循环迭代优化\n",
    "for step, (x, y) in enumerate(train_dataset): \n",
    "    with tf.GradientTape() as tape: # python 语法，对于后续需要释放的资源，使用with...as...语句，避免忘记释放资源\n",
    "        x = tf.reshape(x, (-1, 28*28)) # 打平操作\n",
    "        out = model(x) # 第一步：得到模型输出, 相当于表达式中的Y\n",
    "        # 第二步：计算平均误差\n",
    "        loss = tf.square(out - y) # 计算平方和[b, 10]\n",
    "        loss = tf.reduce_sum(loss) / x.shape[0] # 计算每个样本的平均误差[b]\n",
    "        # 第三步：计算并优化参数[w1, w2, w3, b1, b2, b3]\n",
    "        grads = tape.gradient(loss, model.trainable_variables) # 自动计算梯度\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables)) # w'=w-lr*grad, 更新网络参数\n",
    "        # 每500次计算绘制一次图像\n",
    "#        if step % 100 == 0: \n",
    "#            print(epoch, step, 'loss:', loss.numpy())\n",
    "    print(step, 'loss:', loss.numpy())\n",
    "losses.append(float(loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matplotlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-06d62c234114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 每个epoch次训练，绘制偏移量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'font.size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.titlesize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matplotlib' is not defined"
     ]
    }
   ],
   "source": [
    "# 每个epoch次训练，绘制偏移量\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses, color='C0', marker='s', label='训练')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig('forward.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
