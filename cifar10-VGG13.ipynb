{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n",
      "sample (128, 32, 32, 3) (128,) tf.Tensor(-1.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_54 (Conv2D)           multiple                  1792      \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           multiple                  36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling multiple                  0         \n",
      "=================================================================\n",
      "Total params: 38,720\n",
      "Trainable params: 38,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             multiple                  131328    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 165,514\n",
      "Trainable params: 165,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, Sequential, datasets, optimizers, losses\n",
    "\n",
    "#'''\n",
    "def preprocess(x, y):\n",
    "    x = 2*tf.cast(x, dtype=tf.float32)/255. - 1\n",
    "    y = tf.cast(y, dtype=tf.float32)\n",
    "    return x, y\n",
    "#'''\n",
    "\n",
    "#下载cifar10数据集\n",
    "(x_in, y_in), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "# 构建训练集、测试集对象，随即打乱，预处理，批量化\n",
    "''' # 用以下语句进行预处理，效果与用map(preprocess)函数是一样的\n",
    "x_in = tf.convert_to_tensor(x_in, dtype=tf.float32)/255.\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)/255.\n",
    "y_in = tf.convert_to_tensor(y_in, dtype=tf.int32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_in, y_in))\n",
    "train_db = train_db.shuffle(1000).batch(128)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.batch(128)\n",
    "'''\n",
    "\n",
    "# 打印训练集的形状\n",
    "print(x_in.shape, y_in.shape, x_test.shape, y_test.shape)\n",
    "# 交换维度\n",
    "y_in = tf.squeeze(y_in, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "\n",
    "# 打印训练集的形状\n",
    "print(x_in.shape, y_in.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "#'''\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_in, y_in))\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(128) # 通过map(preprocess)进行数据预处理。\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)\n",
    "#'''\n",
    "\n",
    "\n",
    "# 从训练集中采样一个batch，并观察\n",
    "sample = next(iter(train_db))\n",
    "print('sample', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n",
    "\n",
    "\n",
    "# 创建卷基层，5个\n",
    "conv_layers = [ #创建包含多层网络的列表\n",
    "    # 64个3✖️3卷积核，输入输出同大小\n",
    "    # padding='VALID'时不会对图片做填充；\n",
    "    # 64个核是随机生成的\n",
    "    layers.Conv2D(64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    # 高宽减半\n",
    "    layers.MaxPool2D(pool_size=[2,2], strides=2, padding='same'), \n",
    "\n",
    "    # 128个3✖️3卷积核，输入输出同大小\n",
    "    layers.Conv2D(128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2,2], strides=2, padding='same'), \n",
    "    \n",
    "    # 256个3✖️3卷积核，输入输出同大小\n",
    "    layers.Conv2D(256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2,2], strides=2, padding='same'), \n",
    "    \n",
    "    # 512个3✖️3卷积核，输入输出同大小\n",
    "    layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2,2], strides=2, padding='same'), \n",
    "    \n",
    "    # 512个3✖️3卷积核，输入输出同大小\n",
    "    layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2,2], strides=2, padding='same')\n",
    "]\n",
    "conv_net = Sequential(conv_layers)\n",
    "\n",
    "# 全联接层\n",
    "fc_net = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),\n",
    "    layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(10, activation=None),\n",
    "])\n",
    "\n",
    "# build 两个字网络，并打印参数信息\n",
    "# build 的作用是在训练“fit”之前如果想查看网络结构，可以先用build建立起来网络\n",
    "# build建立的网络不会带有权重，fit建立的是包括了训练的权重的网络\n",
    "conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "fc_net.build(input_shape=[None, 512])\n",
    "conv_net.summary()\n",
    "fc_net.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 优化模型\n",
    "# 设置损失函数、优化器\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "# criteon = keras.losses.CategoricalCrossentropy(from_logits=True) \n",
    "# 合并两个子网络参数，需要在model.build之后才能计算\n",
    "variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 32, 32, 3] ==> [b, 1, 1, 512]\n",
    "            out = conv_net(x)\n",
    "            print(out.shape)\n",
    "            # 打平\n",
    "            out = tf.reshape(out, [-1, 512])\n",
    "            logits = fc_net(out)\n",
    "#            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # calculate losses\n",
    "#            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "#            loss = tf.reduce_mean(loss)\n",
    "        # 计算梯度\n",
    "#        grads = tape.gradient(loss, variables)\n",
    "        # 更新权重\n",
    "#        optimizer.apply_gradients(zip(grads, variables))\n",
    "#        print(step, loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
